{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2958cf33",
   "metadata": {},
   "source": [
    "# 3.Twitter Sentiment Analysis: Training The data With Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e325c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import nlpaug.augmenter.word as naw\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sns.set_style('darkgrid')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad8ff7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03fb6096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3220425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"id\",axis=1,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb1c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes pattern in the input text\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for word in r:\n",
    "        input_txt = re.sub(word, \"\", input_txt)\n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "155f7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove twitter handles (@user)\n",
    "data['clean_tweet'] = np.vectorize(remove_pattern)(data['tweet'], \"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d60bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_documents(document):\n",
    "  stop = stopwords.words('english')\n",
    "  tokens = word_tokenize(document.lower())\n",
    "\n",
    "  word_tokens = [token for token in tokens if token.isalpha()]\n",
    "  clean_tokens = [token for token in word_tokens if token not in stop]\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  lemmatized_tokens = [lemmatizer.lemmatize(token) for token in clean_tokens]\n",
    "\n",
    "  return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07e4543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"clean_tweet\"] = data[\"clean_tweet\"].apply(clean_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16badd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.clean_tweet\n",
    "y=data.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6ca8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26305bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c79502b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18168    russian default position faced accusation dopi...\n",
       "15286    u looked one hairline already receiving tf hai...\n",
       "4964         wow finally see southcitymall felling waiting\n",
       "5373                   always bereft finish something like\n",
       "24201       hispanic amp feel like stomping listen retweet\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6a22699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18168    0\n",
       "15286    0\n",
       "4964     0\n",
       "5373     0\n",
       "24201    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21ee0e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18168</th>\n",
       "      <td>russian default position faced accusation dopi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15286</th>\n",
       "      <td>u looked one hairline already receiving tf hai...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4964</th>\n",
       "      <td>wow finally see southcitymall felling waiting</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5373</th>\n",
       "      <td>always bereft finish something like</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24201</th>\n",
       "      <td>hispanic amp feel like stomping listen retweet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_tweet  label\n",
       "18168  russian default position faced accusation dopi...      0\n",
       "15286  u looked one hairline already receiving tf hai...      0\n",
       "4964       wow finally see southcitymall felling waiting      0\n",
       "5373                 always bereft finish something like      0\n",
       "24201     hispanic amp feel like stomping listen retweet      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.concat([X_train,y_train], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d1c3c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\abhim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19068\n"
     ]
    }
   ],
   "source": [
    " #nplaug\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "data_resampled_nlpaug = data.copy()\n",
    "\n",
    "aug_texts = []\n",
    "minority_data = data_resampled_nlpaug[data_resampled_nlpaug[\"label\"] == 1]\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "texts = minority_data['clean_tweet'].tolist()\n",
    "\n",
    "for text in texts:\n",
    "    augmented_texts = aug.augment(text, n=12)\n",
    "    \n",
    "    for augmented in augmented_texts:\n",
    "        aug_texts.append(augmented)\n",
    "\n",
    "print(len(aug_texts))\n",
    "\n",
    "temp = pd.DataFrame({\n",
    "    'clean_tweet': aug_texts\n",
    "})\n",
    "        \n",
    "temp[\"label\"] = 1\n",
    "        \n",
    "data_resampled_nlpaug = pd.concat([data_resampled_nlpaug, temp], axis=0)\n",
    "data_resampled_nlpaug = data_resampled_nlpaug.reset_index()\n",
    "data_resampled_nlpaug = data_resampled_nlpaug.drop(columns=['index'])\n",
    "del temp, minority_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4976a25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>russian default position faced accusation dopi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u looked one hairline already receiving tf hai...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wow finally see southcitymall felling waiting</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>always bereft finish something like</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hispanic amp feel like stomping listen retweet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41436</th>\n",
       "      <td>come a documented liar lost home calling nazi ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41437</th>\n",
       "      <td>come a documented liar lost base calling natio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41438</th>\n",
       "      <td>come up ampere documented liar lost base calli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41439</th>\n",
       "      <td>make out a documented liar lost base calling n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41440</th>\n",
       "      <td>come amp documented prevaricator lost base cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41441 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_tweet  label\n",
       "0      russian default position faced accusation dopi...      0\n",
       "1      u looked one hairline already receiving tf hai...      0\n",
       "2          wow finally see southcitymall felling waiting      0\n",
       "3                    always bereft finish something like      0\n",
       "4         hispanic amp feel like stomping listen retweet      1\n",
       "...                                                  ...    ...\n",
       "41436  come a documented liar lost home calling nazi ...      1\n",
       "41437  come a documented liar lost base calling natio...      1\n",
       "41438  come up ampere documented liar lost base calli...      1\n",
       "41439  make out a documented liar lost base calling n...      1\n",
       "41440  come amp documented prevaricator lost base cal...      1\n",
       "\n",
       "[41441 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_resampled_nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2d66f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data_resampled_nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e05acffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_tweet    0\n",
       "label          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a881bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df.clean_tweet\n",
    "y_train=df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1abc435",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0de8cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "cv = CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99660932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.08      0.16      8940\n",
      "           1       0.07      1.00      0.14       649\n",
      "\n",
      "    accuracy                           0.15      9589\n",
      "   macro avg       0.54      0.54      0.15      9589\n",
      "weighted avg       0.94      0.15      0.15      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "rfc= RandomForestClassifier()\n",
    "rfc.fit(X_train_cv, y_train) \n",
    "y_pred = rfc.predict(X_test_cv)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da8c65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "tf = TfidfVectorizer(stop_words= 'english', ngram_range= (1,3), lowercase= True, max_features= 5000)\n",
    "X_train_tf = tf.fit_transform(X_train)\n",
    "X_test_tf = tf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b36ed18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      8940\n",
      "           1       0.58      0.68      0.63       649\n",
      "\n",
      "    accuracy                           0.95      9589\n",
      "   macro avg       0.78      0.82      0.80      9589\n",
      "weighted avg       0.95      0.95      0.95      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc= RandomForestClassifier()\n",
    "rfc.fit(X_train_tf, y_train) \n",
    "y_pred = rfc.predict(X_test_tf)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b57af2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96      8940\n",
      "           1       0.51      0.74      0.60       649\n",
      "\n",
      "    accuracy                           0.93      9589\n",
      "   macro avg       0.74      0.84      0.78      9589\n",
      "weighted avg       0.95      0.93      0.94      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "# HARD MARGIN\n",
    "lin=LinearSVC(random_state=42)\n",
    "lin.fit(X_train_tf, y_train)\n",
    "y_pred = lin.predict(X_test_tf)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5889889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, SimpleRNN, Bidirectional, LSTM, GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76aef5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing text\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4f03709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'amp',\n",
       " 2: 'love',\n",
       " 3: 'day',\n",
       " 4: 'u',\n",
       " 5: 'like',\n",
       " 6: 'trump',\n",
       " 7: 'libtard',\n",
       " 8: 'black',\n",
       " 9: 'white',\n",
       " 10: 'people',\n",
       " 11: 'happy',\n",
       " 12: 'new',\n",
       " 13: 'get',\n",
       " 14: 'time',\n",
       " 15: 'woman',\n",
       " 16: 'life',\n",
       " 17: 'good',\n",
       " 18: 'make',\n",
       " 19: 'today',\n",
       " 20: 'racist',\n",
       " 21: 'one',\n",
       " 22: 'go',\n",
       " 23: 'allahsoil',\n",
       " 24: 'feel',\n",
       " 25: 'want',\n",
       " 26: 'see',\n",
       " 27: 'girl',\n",
       " 28: 'politics',\n",
       " 29: 'need',\n",
       " 30: 'ca',\n",
       " 31: 'thankful',\n",
       " 32: 'say',\n",
       " 33: 'retweet',\n",
       " 34: 'father',\n",
       " 35: 'positive',\n",
       " 36: 'obama',\n",
       " 37: 'liberal',\n",
       " 38: 'hate',\n",
       " 39: 'listen',\n",
       " 40: 'sjw',\n",
       " 41: 'year',\n",
       " 42: 'might',\n",
       " 43: 'take',\n",
       " 44: 'think',\n",
       " 45: 'smile',\n",
       " 46: 'would',\n",
       " 47: 'bihday',\n",
       " 48: 'work',\n",
       " 49: 'got',\n",
       " 50: 'america',\n",
       " 51: 'video',\n",
       " 52: 'great',\n",
       " 53: 'friend',\n",
       " 54: 'via',\n",
       " 55: 'look',\n",
       " 56: 'thing',\n",
       " 57: 'way',\n",
       " 58: 'thanks',\n",
       " 59: 'stop',\n",
       " 60: 'really',\n",
       " 61: 'never',\n",
       " 62: 'right',\n",
       " 63: 'family',\n",
       " 64: 'world',\n",
       " 65: 'know',\n",
       " 66: 'back',\n",
       " 67: 'show',\n",
       " 68: 'week',\n",
       " 69: 'man',\n",
       " 70: 'call',\n",
       " 71: 'guy',\n",
       " 72: 'best',\n",
       " 73: 'going',\n",
       " 74: 'much',\n",
       " 75: 'come',\n",
       " 76: 'healthy',\n",
       " 77: 'first',\n",
       " 78: 'weekend',\n",
       " 79: 'american',\n",
       " 80: 'comment',\n",
       " 81: 'fun',\n",
       " 82: 'home',\n",
       " 83: 'racism',\n",
       " 84: 'even',\n",
       " 85: 'summer',\n",
       " 86: 'let',\n",
       " 87: 'bull',\n",
       " 88: 'music',\n",
       " 89: 'live',\n",
       " 90: 'sad',\n",
       " 91: 'many',\n",
       " 92: 'friday',\n",
       " 93: 'still',\n",
       " 94: 'stomping',\n",
       " 95: 'race',\n",
       " 96: 'pay',\n",
       " 97: 'wait',\n",
       " 98: 'hatred',\n",
       " 99: 'men',\n",
       " 100: 'beautiful',\n",
       " 101: 'always',\n",
       " 102: 'hope',\n",
       " 103: 'morning',\n",
       " 104: 'word',\n",
       " 105: 'news',\n",
       " 106: 'sex',\n",
       " 107: 'keep',\n",
       " 108: 'tweet',\n",
       " 109: 'tampa',\n",
       " 110: 'porn',\n",
       " 111: 'school',\n",
       " 112: 'dad',\n",
       " 113: 'find',\n",
       " 114: 'free',\n",
       " 115: 'next',\n",
       " 116: 'nothing',\n",
       " 117: 'well',\n",
       " 118: 'of',\n",
       " 119: 'latest',\n",
       " 120: 'thought',\n",
       " 121: 'last',\n",
       " 122: 'ever',\n",
       " 123: 'wish',\n",
       " 124: 'everyone',\n",
       " 125: 'gt',\n",
       " 126: 'fathersday',\n",
       " 127: 'blog',\n",
       " 128: 'please',\n",
       " 129: 'tomorrow',\n",
       " 130: 'night',\n",
       " 131: 'act',\n",
       " 132: 'orlando',\n",
       " 133: 'give',\n",
       " 134: 'happiness',\n",
       " 135: 'cute',\n",
       " 136: 'child',\n",
       " 137: 'thank',\n",
       " 138: 'model',\n",
       " 139: 'blm',\n",
       " 140: 'sunday',\n",
       " 141: 'affirmation',\n",
       " 142: 'every',\n",
       " 143: 'book',\n",
       " 144: 'game',\n",
       " 145: 'bigot',\n",
       " 146: 'paladino',\n",
       " 147: 'god',\n",
       " 148: 'attack',\n",
       " 149: 'help',\n",
       " 150: 'fuck',\n",
       " 151: 'could',\n",
       " 152: 'kid',\n",
       " 153: 'state',\n",
       " 154: 'little',\n",
       " 155: 'police',\n",
       " 156: 'brexit',\n",
       " 157: 'believe',\n",
       " 158: 'mean',\n",
       " 159: 'follow',\n",
       " 160: 'wo',\n",
       " 161: 'made',\n",
       " 162: 'real',\n",
       " 163: 'another',\n",
       " 164: 'end',\n",
       " 165: 'finally',\n",
       " 166: 'na',\n",
       " 167: 'peace',\n",
       " 168: 'muslim',\n",
       " 169: 'feeling',\n",
       " 170: 'watch',\n",
       " 171: 'team',\n",
       " 172: 'president',\n",
       " 173: 'city',\n",
       " 174: 'boy',\n",
       " 175: 'selfie',\n",
       " 176: 'someone',\n",
       " 177: 'daily',\n",
       " 178: 'malevote',\n",
       " 179: 'old',\n",
       " 180: 'yes',\n",
       " 181: 'quote',\n",
       " 182: 'better',\n",
       " 183: 'suppoers',\n",
       " 184: 'change',\n",
       " 185: 'done',\n",
       " 186: 'medium',\n",
       " 187: 'amazing',\n",
       " 188: 'silver',\n",
       " 189: 'republican',\n",
       " 190: 'tonight',\n",
       " 191: 'place',\n",
       " 192: 'hispanic',\n",
       " 193: 'lot',\n",
       " 194: 'jew',\n",
       " 195: 'lol',\n",
       " 196: 'looking',\n",
       " 197: 'lost',\n",
       " 198: 'person',\n",
       " 199: 'altwaystoheal',\n",
       " 200: 'fan',\n",
       " 201: 'history',\n",
       " 202: 'gold',\n",
       " 203: 'country',\n",
       " 204: 'without',\n",
       " 205: 'food',\n",
       " 206: 'calgary',\n",
       " 207: 'sikh',\n",
       " 208: 'wso',\n",
       " 209: 'condemns',\n",
       " 210: 'big',\n",
       " 211: 'christmas',\n",
       " 212: 'political',\n",
       " 213: 'matter',\n",
       " 214: 'hey',\n",
       " 215: 'face',\n",
       " 216: 'dog',\n",
       " 217: 'maga',\n",
       " 218: 'ready',\n",
       " 219: 'use',\n",
       " 220: 'usa',\n",
       " 221: 'may',\n",
       " 222: 'holiday',\n",
       " 223: 'enjoy',\n",
       " 224: 'misogynist',\n",
       " 225: 'long',\n",
       " 226: 'business',\n",
       " 227: 'baby',\n",
       " 228: 'nice',\n",
       " 229: 'getting',\n",
       " 230: 'check',\n",
       " 231: 'power',\n",
       " 232: 'feminismiscancer',\n",
       " 233: 'feminismisterrorism',\n",
       " 234: 'feminismmuktbharat',\n",
       " 235: 'sta',\n",
       " 236: 'also',\n",
       " 237: 'seashepherd',\n",
       " 238: 'funny',\n",
       " 239: 'play',\n",
       " 240: 'tell',\n",
       " 241: 'left',\n",
       " 242: 'yet',\n",
       " 243: 'enough',\n",
       " 244: 'hour',\n",
       " 245: 'lt',\n",
       " 246: 'truth',\n",
       " 247: 'lady',\n",
       " 248: 'proud',\n",
       " 249: 'coming',\n",
       " 250: 'healing',\n",
       " 251: 'hard',\n",
       " 252: 'already',\n",
       " 253: 'oh',\n",
       " 254: 'antiracism',\n",
       " 255: 'forex',\n",
       " 256: 'nude',\n",
       " 257: 'ur',\n",
       " 258: 'as',\n",
       " 259: 'christian',\n",
       " 260: 'whatever',\n",
       " 261: 'sea',\n",
       " 262: 'sun',\n",
       " 263: 'job',\n",
       " 264: 'money',\n",
       " 265: 'around',\n",
       " 266: 'hot',\n",
       " 267: 'others',\n",
       " 268: 'bad',\n",
       " 269: 'post',\n",
       " 270: 'picture',\n",
       " 271: 'true',\n",
       " 272: 'im',\n",
       " 273: 'pa',\n",
       " 274: 'blessed',\n",
       " 275: 'blacklivesmatter',\n",
       " 276: 'away',\n",
       " 277: 'shit',\n",
       " 278: 'read',\n",
       " 279: 'carl',\n",
       " 280: 'remark',\n",
       " 281: 'found',\n",
       " 282: 'said',\n",
       " 283: 'ppl',\n",
       " 284: 'a',\n",
       " 285: 'wow',\n",
       " 286: 'ignored',\n",
       " 287: 'far',\n",
       " 288: 'win',\n",
       " 289: 'bear',\n",
       " 290: 'miss',\n",
       " 291: 'kkk',\n",
       " 292: 'saying',\n",
       " 293: 'the',\n",
       " 294: 'saturday',\n",
       " 295: 'photo',\n",
       " 296: 'class',\n",
       " 297: 'racialist',\n",
       " 298: 'body',\n",
       " 299: 'angry',\n",
       " 300: 'death',\n",
       " 301: 'everything',\n",
       " 302: 'uk',\n",
       " 303: 'buffalo',\n",
       " 304: 'climb',\n",
       " 305: 'instagood',\n",
       " 306: 'leadership',\n",
       " 307: 'altright',\n",
       " 308: 'carlpaladino',\n",
       " 309: 'young',\n",
       " 310: 'asian',\n",
       " 311: 'color',\n",
       " 312: 'must',\n",
       " 313: 'oil',\n",
       " 314: 'waiting',\n",
       " 315: 'house',\n",
       " 316: 'temple',\n",
       " 317: 'leave',\n",
       " 318: 'war',\n",
       " 319: 'discrimination',\n",
       " 320: 'beach',\n",
       " 321: 'story',\n",
       " 322: 'month',\n",
       " 323: 'rest',\n",
       " 324: 'speak',\n",
       " 325: 'gon',\n",
       " 326: 'ally',\n",
       " 327: 'homophobic',\n",
       " 328: 'cool',\n",
       " 329: 'n',\n",
       " 330: 'card',\n",
       " 331: 'nazi',\n",
       " 332: 'gorilla',\n",
       " 333: 'vandalised',\n",
       " 334: 'newyork',\n",
       " 335: 'cat',\n",
       " 336: 'wedding',\n",
       " 337: 'mind',\n",
       " 338: 'sexy',\n",
       " 339: 'excited',\n",
       " 340: 'run',\n",
       " 341: 'boricua',\n",
       " 342: 'teambts',\n",
       " 343: 'r',\n",
       " 344: 'direct',\n",
       " 345: 'called',\n",
       " 346: 'equality',\n",
       " 347: 'kind',\n",
       " 348: 'human',\n",
       " 349: 'flag',\n",
       " 350: 'sick',\n",
       " 351: 'polar',\n",
       " 352: 'shepherd',\n",
       " 353: 'something',\n",
       " 354: 'awesome',\n",
       " 355: 'dominate',\n",
       " 356: 'racial',\n",
       " 357: 'twitter',\n",
       " 358: 'son',\n",
       " 359: 'full',\n",
       " 360: 'male',\n",
       " 361: 'watching',\n",
       " 362: 'try',\n",
       " 363: 'hea',\n",
       " 364: 'crazy',\n",
       " 365: 'fact',\n",
       " 366: 'boycott',\n",
       " 367: 'lie',\n",
       " 368: 'hillary',\n",
       " 369: 'yeah',\n",
       " 370: 'japan',\n",
       " 371: 'officer',\n",
       " 372: 'buy',\n",
       " 373: 'soon',\n",
       " 374: 'two',\n",
       " 375: 'gay',\n",
       " 376: 'join',\n",
       " 377: 'victim',\n",
       " 378: 'feminism',\n",
       " 379: 'talking',\n",
       " 380: 'sleep',\n",
       " 381: 'notmypresident',\n",
       " 382: 'obamas',\n",
       " 383: 'emiratis',\n",
       " 384: 'playing',\n",
       " 385: 'action',\n",
       " 386: 'motivation',\n",
       " 387: 'tear',\n",
       " 388: 'reason',\n",
       " 389: 'worst',\n",
       " 390: 'stay',\n",
       " 391: 'message',\n",
       " 392: 'hair',\n",
       " 393: 'monday',\n",
       " 394: 'miami',\n",
       " 395: 'reading',\n",
       " 396: 'ignorance',\n",
       " 397: 'hater',\n",
       " 398: 'factory',\n",
       " 399: 'grateful',\n",
       " 400: 'fucking',\n",
       " 401: 'hand',\n",
       " 402: 'moment',\n",
       " 403: 'yr',\n",
       " 404: 'single',\n",
       " 405: 'forward',\n",
       " 406: 'vote',\n",
       " 407: 'african',\n",
       " 408: 'lovely',\n",
       " 409: 'fashion',\n",
       " 410: 'travel',\n",
       " 411: 'female',\n",
       " 412: 'aicle',\n",
       " 413: 'cnn',\n",
       " 414: 'ampere',\n",
       " 415: 'shooting',\n",
       " 416: 'number',\n",
       " 417: 'omg',\n",
       " 418: 'dream',\n",
       " 419: 'naked',\n",
       " 420: 'michelle',\n",
       " 421: 'terrorist',\n",
       " 422: 'kill',\n",
       " 423: 'lgbt',\n",
       " 424: 'put',\n",
       " 425: 'thats',\n",
       " 426: 'followme',\n",
       " 427: 'beauty',\n",
       " 428: 'parent',\n",
       " 429: 'national',\n",
       " 430: 'gun',\n",
       " 431: 'fascist',\n",
       " 432: 'xenophobia',\n",
       " 433: 'southafrica',\n",
       " 434: 'talk',\n",
       " 435: 'justice',\n",
       " 436: 'course',\n",
       " 437: 'making',\n",
       " 438: 'fuhered',\n",
       " 439: 'mother',\n",
       " 440: 'working',\n",
       " 441: 'mad',\n",
       " 442: 'putinschoice',\n",
       " 443: 'fascism',\n",
       " 444: 'anyone',\n",
       " 445: 'bit',\n",
       " 446: 'photooftheday',\n",
       " 447: 'fear',\n",
       " 448: 'song',\n",
       " 449: 'perfect',\n",
       " 450: 'group',\n",
       " 451: 'suppo',\n",
       " 452: 'trying',\n",
       " 453: 'le',\n",
       " 454: 'adenosine',\n",
       " 455: 'monophosphate',\n",
       " 456: 'couple',\n",
       " 457: 'open',\n",
       " 458: 'anything',\n",
       " 459: 'health',\n",
       " 460: 'problem',\n",
       " 461: 'similar',\n",
       " 462: 'order',\n",
       " 463: 'resist',\n",
       " 464: 'strong',\n",
       " 465: 'football',\n",
       " 466: 'gop',\n",
       " 467: 'b',\n",
       " 468: 'social',\n",
       " 469: 'in',\n",
       " 470: 'list',\n",
       " 471: 'joy',\n",
       " 472: 'care',\n",
       " 473: 'pretty',\n",
       " 474: 'since',\n",
       " 475: 'claim',\n",
       " 476: 'movie',\n",
       " 477: 'service',\n",
       " 478: 'together',\n",
       " 479: 'bing',\n",
       " 480: 'policy',\n",
       " 481: 'meet',\n",
       " 482: 'smh',\n",
       " 483: 'rip',\n",
       " 484: 'adenylic',\n",
       " 485: 'acid',\n",
       " 486: 'trumpet',\n",
       " 487: 'joke',\n",
       " 488: 'gone',\n",
       " 489: 'impoant',\n",
       " 490: 'ugly',\n",
       " 491: 'bong',\n",
       " 492: 'education',\n",
       " 493: 'terrorism',\n",
       " 494: 'pic',\n",
       " 495: 'united',\n",
       " 496: 'dear',\n",
       " 497: 'religion',\n",
       " 498: 'understand',\n",
       " 499: 'cry',\n",
       " 500: 'calling',\n",
       " 501: 'share',\n",
       " 502: 'lead',\n",
       " 503: 'shot',\n",
       " 504: 'newyear',\n",
       " 505: 'sorry',\n",
       " 506: 'late',\n",
       " 507: 'fight',\n",
       " 508: 'turn',\n",
       " 509: 'stopracism',\n",
       " 510: 'freedom',\n",
       " 511: 'cause',\n",
       " 512: 'million',\n",
       " 513: 'whitepeople',\n",
       " 514: 'islamophobia',\n",
       " 515: 'science',\n",
       " 516: 'misogyny',\n",
       " 517: 'experience',\n",
       " 518: 'tv',\n",
       " 519: 'mass',\n",
       " 520: 'truly',\n",
       " 521: 'pig',\n",
       " 522: 'election',\n",
       " 523: 'land',\n",
       " 524: 'ago',\n",
       " 525: 'cold',\n",
       " 526: 'racing',\n",
       " 527: 'shop',\n",
       " 528: 'out',\n",
       " 529: 'dead',\n",
       " 530: 'head',\n",
       " 531: 'june',\n",
       " 532: 'age',\n",
       " 533: 'bigotry',\n",
       " 534: 'law',\n",
       " 535: 'hear',\n",
       " 536: 'towards',\n",
       " 537: 'inspiration',\n",
       " 538: 'dangerous',\n",
       " 539: 'line',\n",
       " 540: 'chick',\n",
       " 541: 'india',\n",
       " 542: 'surprise',\n",
       " 543: 'government',\n",
       " 544: 'nyc',\n",
       " 545: 'wrong',\n",
       " 546: 'impression',\n",
       " 547: 'student',\n",
       " 548: 'else',\n",
       " 549: 'wall',\n",
       " 550: 'point',\n",
       " 551: 'stand',\n",
       " 552: 'deal',\n",
       " 553: 'idea',\n",
       " 554: 'though',\n",
       " 555: 'value',\n",
       " 556: 'leader',\n",
       " 557: 'rape',\n",
       " 558: 'accept',\n",
       " 559: 'answer',\n",
       " 560: 'tcot',\n",
       " 561: 'v',\n",
       " 562: 'member',\n",
       " 563: 'actually',\n",
       " 564: 'present',\n",
       " 565: 'happened',\n",
       " 566: 'become',\n",
       " 567: 'term',\n",
       " 568: 'poetry',\n",
       " 569: 'russia',\n",
       " 570: 'eye',\n",
       " 571: 'nigger',\n",
       " 572: 'immigrant',\n",
       " 573: 'success',\n",
       " 574: 'dance',\n",
       " 575: 'plan',\n",
       " 576: 'f',\n",
       " 577: 'celebrate',\n",
       " 578: 'church',\n",
       " 579: 'board',\n",
       " 580: 'coffee',\n",
       " 581: 'remember',\n",
       " 582: 'season',\n",
       " 583: 'animal',\n",
       " 584: 'york',\n",
       " 585: 'session',\n",
       " 586: 'stereotype',\n",
       " 587: 'putin',\n",
       " 588: 'up',\n",
       " 589: 'donald',\n",
       " 590: 'sign',\n",
       " 591: 'depression',\n",
       " 592: 'event',\n",
       " 593: 'community',\n",
       " 594: 'disappointed',\n",
       " 595: 'account',\n",
       " 596: 'delete',\n",
       " 597: 'living',\n",
       " 598: 'theresistance',\n",
       " 599: 'tbt',\n",
       " 600: 'green',\n",
       " 601: 'liar',\n",
       " 602: 'tech',\n",
       " 603: 'half',\n",
       " 604: 'hold',\n",
       " 605: 'society',\n",
       " 606: 'send',\n",
       " 607: 'latino',\n",
       " 608: 'meme',\n",
       " 609: 'anti',\n",
       " 610: 'london',\n",
       " 611: 'future',\n",
       " 612: 'blue',\n",
       " 613: 'laugh',\n",
       " 614: 'wtf',\n",
       " 615: 'wonderful',\n",
       " 616: 'abuse',\n",
       " 617: 'eah',\n",
       " 618: 'horn',\n",
       " 619: 'target',\n",
       " 620: 'nation',\n",
       " 621: 'customer',\n",
       " 622: 'israel',\n",
       " 623: 'profiling',\n",
       " 624: 'fly',\n",
       " 625: 'different',\n",
       " 626: 'loving',\n",
       " 627: 'fought',\n",
       " 628: 'dont',\n",
       " 629: 'super',\n",
       " 630: 'x',\n",
       " 631: 'bitch',\n",
       " 632: 'car',\n",
       " 633: 'protesting',\n",
       " 634: 'used',\n",
       " 635: 'loved',\n",
       " 636: 'australia',\n",
       " 637: 'heard',\n",
       " 638: 'side',\n",
       " 639: 'mom',\n",
       " 640: 'sound',\n",
       " 641: 'ticket',\n",
       " 642: 'self',\n",
       " 643: 'mindset',\n",
       " 644: 'campaign',\n",
       " 645: 'office',\n",
       " 646: 'name',\n",
       " 647: 'minute',\n",
       " 648: 'fitness',\n",
       " 649: 'high',\n",
       " 650: 'hollywood',\n",
       " 651: 'fired',\n",
       " 652: 'thursday',\n",
       " 653: 'came',\n",
       " 654: 'flower',\n",
       " 655: 'klan',\n",
       " 656: 'preorder',\n",
       " 657: 'least',\n",
       " 658: 'forever',\n",
       " 659: 'fucked',\n",
       " 660: 'piece',\n",
       " 661: 'idiot',\n",
       " 662: 'set',\n",
       " 663: 'voted',\n",
       " 664: 'definition',\n",
       " 665: 'cornet',\n",
       " 666: 'environmental',\n",
       " 667: 'market',\n",
       " 668: 'bring',\n",
       " 669: 'lifestyle',\n",
       " 670: 'guess',\n",
       " 671: 'teen',\n",
       " 672: 'break',\n",
       " 673: 'belief',\n",
       " 674: 'total',\n",
       " 675: 'cant',\n",
       " 676: 'denial',\n",
       " 677: 'mood',\n",
       " 678: 'drink',\n",
       " 679: 'traitor',\n",
       " 680: 'walk',\n",
       " 681: 'following',\n",
       " 682: 'issue',\n",
       " 683: 'tired',\n",
       " 684: 'save',\n",
       " 685: 'till',\n",
       " 686: 'woh',\n",
       " 687: 'anymore',\n",
       " 688: 'record',\n",
       " 689: 'meant',\n",
       " 690: 'yay',\n",
       " 691: 'ask',\n",
       " 692: 'move',\n",
       " 693: 'daughter',\n",
       " 694: 'daddy',\n",
       " 695: 'cleveland',\n",
       " 696: 'bed',\n",
       " 697: 'maybe',\n",
       " 698: 'moron',\n",
       " 699: 'close',\n",
       " 700: 'cow',\n",
       " 701: 'eat',\n",
       " 702: 'bc',\n",
       " 703: 'alone',\n",
       " 704: 'conference',\n",
       " 705: 'ny',\n",
       " 706: 'sexism',\n",
       " 707: 'trip',\n",
       " 708: 'seeing',\n",
       " 709: 'auspol',\n",
       " 710: 'ok',\n",
       " 711: 'official',\n",
       " 712: 'sweet',\n",
       " 713: 'question',\n",
       " 714: 'fall',\n",
       " 715: 'sure',\n",
       " 716: 'bird',\n",
       " 717: 'feminist',\n",
       " 718: 'palestinian',\n",
       " 719: 'environment',\n",
       " 720: 'milo',\n",
       " 721: 'phillysuppophilly',\n",
       " 722: 'newswithed',\n",
       " 723: 'fakenews',\n",
       " 724: 'promote',\n",
       " 725: 'michelleobama',\n",
       " 726: 'brown',\n",
       " 727: 'almost',\n",
       " 728: 'street',\n",
       " 729: 'fresh',\n",
       " 730: 'troll',\n",
       " 731: 'telling',\n",
       " 732: 'stupid',\n",
       " 733: 'gender',\n",
       " 734: 'yo',\n",
       " 735: 'trash',\n",
       " 736: 'choice',\n",
       " 737: 'islam',\n",
       " 738: 'military',\n",
       " 739: 'brother',\n",
       " 740: 'seriously',\n",
       " 741: 'forget',\n",
       " 742: 'happening',\n",
       " 743: 'lying',\n",
       " 744: 'relax',\n",
       " 745: 'prayer',\n",
       " 746: 'entire',\n",
       " 747: 'damn',\n",
       " 748: 'asshole',\n",
       " 749: 'adult',\n",
       " 750: 'yesterday',\n",
       " 751: 'africa',\n",
       " 752: 'seen',\n",
       " 753: 'flight',\n",
       " 754: 'dark',\n",
       " 755: 'nationalist',\n",
       " 756: 'assault',\n",
       " 757: 'simulator',\n",
       " 758: 'adapt',\n",
       " 759: 'star',\n",
       " 760: 'past',\n",
       " 761: 'die',\n",
       " 762: 'cop',\n",
       " 763: 'relationship',\n",
       " 764: 'islamic',\n",
       " 765: 'hell',\n",
       " 766: 'cologne',\n",
       " 767: 'humanity',\n",
       " 768: 'hateful',\n",
       " 769: 'instagram',\n",
       " 770: 'date',\n",
       " 771: 'tho',\n",
       " 772: 'gift',\n",
       " 773: 'safe',\n",
       " 774: 'vacation',\n",
       " 775: 'light',\n",
       " 776: 'content',\n",
       " 777: 'nasty',\n",
       " 778: 'phone',\n",
       " 779: 'lose',\n",
       " 780: 'disgusting',\n",
       " 781: 'top',\n",
       " 782: 'clean',\n",
       " 783: 'cleaning',\n",
       " 784: 'able',\n",
       " 785: 'company',\n",
       " 786: 'within',\n",
       " 787: 'whole',\n",
       " 788: 'despite',\n",
       " 789: 'practice',\n",
       " 790: 'pick',\n",
       " 791: 'went',\n",
       " 792: 'complete',\n",
       " 793: 'xxx',\n",
       " 794: 'cheer',\n",
       " 795: 'refugee',\n",
       " 796: 'hit',\n",
       " 797: 'running',\n",
       " 798: 'knew',\n",
       " 799: 'told',\n",
       " 800: 'culture',\n",
       " 801: 'protest',\n",
       " 802: 'meeting',\n",
       " 803: 'fair',\n",
       " 804: 'using',\n",
       " 805: 'film',\n",
       " 806: 'industry',\n",
       " 807: 'ignorant',\n",
       " 808: 'room',\n",
       " 809: 'treat',\n",
       " 810: 'seems',\n",
       " 811: 'shame',\n",
       " 812: 'view',\n",
       " 813: 'hello',\n",
       " 814: 'wednesday',\n",
       " 815: 'fire',\n",
       " 816: 'system',\n",
       " 817: 'begin',\n",
       " 818: 'marijuana',\n",
       " 819: 'movement',\n",
       " 820: 'ag',\n",
       " 821: 'evil',\n",
       " 822: 'saw',\n",
       " 823: 'empty',\n",
       " 824: 'agree',\n",
       " 825: 'systemic',\n",
       " 826: 'simulation',\n",
       " 827: 'sunshine',\n",
       " 828: 'sexual',\n",
       " 829: 'block',\n",
       " 830: 'mc',\n",
       " 831: 'korean',\n",
       " 832: 'special',\n",
       " 833: 'red',\n",
       " 834: 'povey',\n",
       " 835: 'app',\n",
       " 836: 'leaving',\n",
       " 837: 'early',\n",
       " 838: 'tgif',\n",
       " 839: 'stuff',\n",
       " 840: 'update',\n",
       " 841: 'given',\n",
       " 842: 'voice',\n",
       " 843: 'disease',\n",
       " 844: 'sexist',\n",
       " 845: 'reality',\n",
       " 846: 'welcome',\n",
       " 847: 'suppoing',\n",
       " 848: 'humanrights',\n",
       " 849: 'suck',\n",
       " 850: 'ibooks',\n",
       " 851: 'extremist',\n",
       " 852: 'xenophobic',\n",
       " 853: 'propaganda',\n",
       " 854: 'states',\n",
       " 855: 'on',\n",
       " 856: 'nigga',\n",
       " 857: 'walking',\n",
       " 858: 'low',\n",
       " 859: 'step',\n",
       " 860: 'etc',\n",
       " 861: 'arab',\n",
       " 862: 'bbc',\n",
       " 863: 'visit',\n",
       " 864: 'trust',\n",
       " 865: 'shut',\n",
       " 866: 'realize',\n",
       " 867: 'teamsuperjunior',\n",
       " 868: 'originally',\n",
       " 869: 'landholding',\n",
       " 870: 'gym',\n",
       " 871: 'cake',\n",
       " 872: 'university',\n",
       " 873: 'freemilo',\n",
       " 874: 'resign',\n",
       " 875: 'whiteprivilege',\n",
       " 876: 'inequality',\n",
       " 877: 'allow',\n",
       " 878: 'dystopian',\n",
       " 879: 'insane',\n",
       " 880: 'fang',\n",
       " 881: 'hatespeech',\n",
       " 882: 'arabic',\n",
       " 883: 'jeffsessions',\n",
       " 884: 'tedtalks',\n",
       " 885: 'mightiness',\n",
       " 886: 'soul',\n",
       " 887: 'useful',\n",
       " 888: 'space',\n",
       " 889: 'shopping',\n",
       " 890: 'fail',\n",
       " 891: 'haha',\n",
       " 892: 'needed',\n",
       " 893: 'wife',\n",
       " 894: 'photography',\n",
       " 895: 'small',\n",
       " 896: 'reaction',\n",
       " 897: 'eve',\n",
       " 898: 'ball',\n",
       " 899: 'plz',\n",
       " 900: 'same',\n",
       " 901: \"ne'er\",\n",
       " 902: 'moslem',\n",
       " 903: 'polarisation',\n",
       " 904: 'mt',\n",
       " 905: 'destroy',\n",
       " 906: 'aww',\n",
       " 907: 'ad',\n",
       " 908: 'town',\n",
       " 909: 'scum',\n",
       " 910: 'favorite',\n",
       " 911: 'program',\n",
       " 912: 'wan',\n",
       " 913: 'denounce',\n",
       " 914: 'water',\n",
       " 915: 'simple',\n",
       " 916: 'style',\n",
       " 917: 'vine',\n",
       " 918: 'episode',\n",
       " 919: 'officially',\n",
       " 920: 'youtube',\n",
       " 921: 'fool',\n",
       " 922: 'combat',\n",
       " 923: 'spark',\n",
       " 924: 'loss',\n",
       " 925: 'queen',\n",
       " 926: 'bully',\n",
       " 927: 'faith',\n",
       " 928: 'attention',\n",
       " 929: 'final',\n",
       " 930: 'sale',\n",
       " 931: 'follower',\n",
       " 932: 'tuesday',\n",
       " 933: 'goal',\n",
       " 934: 'deletetweets',\n",
       " 935: 'silence',\n",
       " 936: 'tip',\n",
       " 937: 'form',\n",
       " 938: 'criminal',\n",
       " 939: 'atomic',\n",
       " 940: 'goodmorning',\n",
       " 941: 'prayfororlando',\n",
       " 942: 'control',\n",
       " 943: 'due',\n",
       " 944: 'instead',\n",
       " 945: 'choose',\n",
       " 946: 'hu',\n",
       " 947: 'nature',\n",
       " 948: 'organization',\n",
       " 949: 'press',\n",
       " 950: 'husband',\n",
       " 951: 'unleashed',\n",
       " 952: 'rather',\n",
       " 953: 'offer',\n",
       " 954: 'mil',\n",
       " 955: 'response',\n",
       " 956: 'citizenry',\n",
       " 957: 'racialism',\n",
       " 958: 'la',\n",
       " 959: 'ai',\n",
       " 960: 'winner',\n",
       " 961: 'rid',\n",
       " 962: 'speech',\n",
       " 963: 'wonder',\n",
       " 964: 'client',\n",
       " 965: 'wear',\n",
       " 966: 'whilst',\n",
       " 967: 'hardcore',\n",
       " 968: 'antisemitism',\n",
       " 969: 'judge',\n",
       " 970: 'purpose',\n",
       " 971: 'nervous',\n",
       " 972: 'ta',\n",
       " 973: 'spread',\n",
       " 974: 'w',\n",
       " 975: 'clinton',\n",
       " 976: 'immigration',\n",
       " 977: 'donkey',\n",
       " 978: 'semitic',\n",
       " 979: 'desire',\n",
       " 980: 'modern',\n",
       " 981: 'intelligence',\n",
       " 982: 'non',\n",
       " 983: 'everyday',\n",
       " 984: 'adam',\n",
       " 985: 'staing',\n",
       " 986: 'lunch',\n",
       " 987: 'bag',\n",
       " 988: 'evening',\n",
       " 989: 'smell',\n",
       " 990: 'anxiety',\n",
       " 991: 'speaking',\n",
       " 992: 'learn',\n",
       " 993: 'genocide',\n",
       " 994: 'actor',\n",
       " 995: 'pussy',\n",
       " 996: 'rock',\n",
       " 997: 'bar',\n",
       " 998: 'along',\n",
       " 999: 'type',\n",
       " 1000: 'cut',\n",
       " ...}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = tokenizer.index_word\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd8289cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29468"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len = len(vocabulary)\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c707c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "535365d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len = []\n",
    "for doc in train_sequence:\n",
    "    doc_len.append(len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a5f027a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(doc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c035179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(doc_len, 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1018ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f44a7524",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence_matrix = sequence.pad_sequences(train_sequence, maxlen= max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa57bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "test_sequence = tokenizer.texts_to_sequences(X_test)\n",
    "test_sequence_matrix = sequence.pad_sequences(test_sequence, maxlen= max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26d51f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_len+1,output_dim=100,input_length=max_len,mask_zero=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation=\"tanh\"))\n",
    "model.add(Dense(64,activation=\"tanh\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03be459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "130456eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1296/1296 [==============================] - 44s 34ms/step - loss: 0.1104\n",
      "Epoch 2/10\n",
      "1296/1296 [==============================] - 40s 31ms/step - loss: 0.0072\n",
      "Epoch 3/10\n",
      "1296/1296 [==============================] - 40s 31ms/step - loss: 0.0035\n",
      "Epoch 4/10\n",
      "1296/1296 [==============================] - 40s 31ms/step - loss: 0.0028\n",
      "Epoch 5/10\n",
      "1296/1296 [==============================] - 40s 31ms/step - loss: 0.0026\n",
      "Epoch 6/10\n",
      "1296/1296 [==============================] - 40s 31ms/step - loss: 0.0018\n",
      "Epoch 7/10\n",
      "1296/1296 [==============================] - 40s 31ms/step - loss: 0.0015\n",
      "Epoch 8/10\n",
      "1296/1296 [==============================] - 42s 33ms/step - loss: 0.0017\n",
      "Epoch 9/10\n",
      "1296/1296 [==============================] - 39s 30ms/step - loss: 0.0013\n",
      "Epoch 10/10\n",
      "1296/1296 [==============================] - 40s 31ms/step - loss: 0.0012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b10649128>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sequence_matrix,y_train, batch_size=32,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbaf93e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      8940\n",
      "           1       0.57      0.58      0.57       649\n",
      "\n",
      "    accuracy                           0.94      9589\n",
      "   macro avg       0.77      0.77      0.77      9589\n",
      "weighted avg       0.94      0.94      0.94      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_sequence_matrix)\n",
    "y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bfd018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-directional\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_len+1,output_dim=100,input_length=max_len,mask_zero=True))\n",
    "model.add(Bidirectional(SimpleRNN(32, activation=\"tanh\")))\n",
    "model.add(Dense(64,activation=\"tanh\"))\n",
    "model.add(Dense(64,activation=\"tanh\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29cb173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1296/1296 [==============================] - 62s 46ms/step - loss: 0.0999\n",
      "Epoch 2/10\n",
      "1296/1296 [==============================] - 83s 64ms/step - loss: 0.0056 0s - loss\n",
      "Epoch 3/10\n",
      "1296/1296 [==============================] - 83s 64ms/step - loss: 0.0042 0s - loss: 0.00\n",
      "Epoch 4/10\n",
      "1296/1296 [==============================] - 86s 66ms/step - loss: 0.0031\n",
      "Epoch 5/10\n",
      "1296/1296 [==============================] - 89s 69ms/step - loss: 0.0040 0s - loss: 0.004\n",
      "Epoch 6/10\n",
      "1296/1296 [==============================] - 85s 66ms/step - loss: 0.0021\n",
      "Epoch 7/10\n",
      "1296/1296 [==============================] - 94s 72ms/step - loss: 9.2554e-04\n",
      "Epoch 8/10\n",
      "1296/1296 [==============================] - 93s 72ms/step - loss: 0.0011\n",
      "Epoch 9/10\n",
      "1296/1296 [==============================] - 88s 68ms/step - loss: 0.0012\n",
      "Epoch 10/10\n",
      "1296/1296 [==============================] - 92s 71ms/step - loss: 0.0025- ETA: 0s - loss: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b11932a90>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "model.fit(train_sequence_matrix,y_train, batch_size=32,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "facd3a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      8940\n",
      "           1       0.63      0.58      0.61       649\n",
      "\n",
      "    accuracy                           0.95      9589\n",
      "   macro avg       0.80      0.78      0.79      9589\n",
      "weighted avg       0.95      0.95      0.95      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_sequence_matrix)\n",
    "y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "765918dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1296/1296 [==============================] - 144s 98ms/step - loss: 0.1213\n",
      "Epoch 2/10\n",
      "1296/1296 [==============================] - 133s 102ms/step - loss: 0.0194s - - ETA: 0s - loss\n",
      "Epoch 3/10\n",
      "1296/1296 [==============================] - 133s 103ms/step - loss: 0.0093s - l\n",
      "Epoch 4/10\n",
      "1296/1296 [==============================] - 125s 96ms/step - loss: 0.00681\n",
      "Epoch 5/10\n",
      "1296/1296 [==============================] - 122s 94ms/step - loss: 0.0043\n",
      "Epoch 6/10\n",
      "1296/1296 [==============================] - 173s 133ms/step - loss: 0.0047\n",
      "Epoch 7/10\n",
      "1296/1296 [==============================] - 169s 130ms/step - loss: 0.0032\n",
      "Epoch 8/10\n",
      "1296/1296 [==============================] - 175s 135ms/step - loss: 0.0034\n",
      "Epoch 9/10\n",
      "1296/1296 [==============================] - 174s 134ms/step - loss: 0.0029\n",
      "Epoch 10/10\n",
      "1296/1296 [==============================] - 167s 129ms/step - loss: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b12282c88>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_len+1,output_dim=100,input_length=max_len,mask_zero=True))\n",
    "model.add(LSTM(128, activation=\"tanh\"))\n",
    "model.add(Dense(128,activation=\"tanh\"))\n",
    "model.add(Dense(64,activation=\"tanh\"))\n",
    "model.add(Dense(64,activation=\"tanh\"))\n",
    "model.add(Dense(64,activation=\"tanh\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "model.fit(train_sequence_matrix,y_train, batch_size=32,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a1e859e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      8940\n",
      "           1       0.60      0.61      0.61       649\n",
      "\n",
      "    accuracy                           0.95      9589\n",
      "   macro avg       0.79      0.79      0.79      9589\n",
      "weighted avg       0.95      0.95      0.95      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_sequence_matrix)\n",
    "y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7ad8ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1296/1296 [==============================] - 217s 131ms/step - loss: 0.1210\n",
      "Epoch 2/10\n",
      "1296/1296 [==============================] - 94s 72ms/step - loss: 0.0175\n",
      "Epoch 3/10\n",
      "1296/1296 [==============================] - 89s 68ms/step - loss: 0.0095\n",
      "Epoch 4/10\n",
      "1296/1296 [==============================] - 100s 77ms/step - loss: 0.0045\n",
      "Epoch 5/10\n",
      "1296/1296 [==============================] - 97s 75ms/step - loss: 0.0035\n",
      "Epoch 6/10\n",
      "1296/1296 [==============================] - 101s 78ms/step - loss: 0.0027\n",
      "Epoch 7/10\n",
      "1296/1296 [==============================] - 94s 72ms/step - loss: 0.0029\n",
      "Epoch 8/10\n",
      "1296/1296 [==============================] - 98s 76ms/step - loss: 0.0029\n",
      "Epoch 9/10\n",
      "1296/1296 [==============================] - 90s 70ms/step - loss: 0.0022\n",
      "Epoch 10/10\n",
      "1296/1296 [==============================] - 89s 69ms/step - loss: 0.0013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b1408aef0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multilayer LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_len+1,output_dim=100,input_length=max_len,mask_zero=True))\n",
    "model.add(LSTM(32, activation=\"tanh\", return_sequences=True))\n",
    "model.add(LSTM(32, activation=\"tanh\"))\n",
    "model.add(Dense(64,activation=\"tanh\"))\n",
    "model.add(Dense(64,activation=\"tanh\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "model.fit(train_sequence_matrix,y_train, batch_size=32,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d7e769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      8940\n",
      "           1       0.70      0.56      0.62       649\n",
      "\n",
      "    accuracy                           0.95      9589\n",
      "   macro avg       0.83      0.77      0.80      9589\n",
      "weighted avg       0.95      0.95      0.95      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_sequence_matrix)\n",
    "y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e60a656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1296/1296 [==============================] - 111s 58ms/step - loss: 0.1123\n",
      "Epoch 2/10\n",
      "1296/1296 [==============================] - 73s 56ms/step - loss: 0.0121\n",
      "Epoch 3/10\n",
      "1296/1296 [==============================] - 78s 60ms/step - loss: 0.0050\n",
      "Epoch 4/10\n",
      "1296/1296 [==============================] - 76s 59ms/step - loss: 0.0039\n",
      "Epoch 5/10\n",
      "1296/1296 [==============================] - 75s 58ms/step - loss: 0.0031\n",
      "Epoch 6/10\n",
      "1296/1296 [==============================] - 81s 62ms/step - loss: 0.0023\n",
      "Epoch 7/10\n",
      "1296/1296 [==============================] - 78s 60ms/step - loss: 0.0013\n",
      "Epoch 8/10\n",
      "1296/1296 [==============================] - 78s 60ms/step - loss: 0.0014\n",
      "Epoch 9/10\n",
      "1296/1296 [==============================] - 74s 57ms/step - loss: 0.0015\n",
      "Epoch 10/10\n",
      "1296/1296 [==============================] - 75s 58ms/step - loss: 0.0013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b13e45160>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bidirection LSTM\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_len+1,output_dim=100,input_length=max_len,mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(32, activation=\"tanh\")))\n",
    "model.add(Dense(64,activation=\"tanh\"))\n",
    "model.add(Dense(64,activation=\"tanh\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "model.fit(train_sequence_matrix,y_train, batch_size=32,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "080e63a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      8940\n",
      "           1       0.61      0.63      0.62       649\n",
      "\n",
      "    accuracy                           0.95      9589\n",
      "   macro avg       0.79      0.80      0.80      9589\n",
      "weighted avg       0.95      0.95      0.95      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_sequence_matrix)\n",
    "y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54704b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "648/648 [==============================] - 77s 80ms/step - loss: 0.1173\n",
      "Epoch 2/50\n",
      "648/648 [==============================] - 51s 79ms/step - loss: 0.0131\n",
      "Epoch 3/50\n",
      "648/648 [==============================] - 52s 80ms/step - loss: 0.0049\n",
      "Epoch 4/50\n",
      "648/648 [==============================] - 52s 80ms/step - loss: 0.0029\n",
      "Epoch 5/50\n",
      "648/648 [==============================] - 53s 82ms/step - loss: 0.0027\n",
      "Epoch 6/50\n",
      "648/648 [==============================] - 54s 83ms/step - loss: 0.0050\n",
      "Epoch 7/50\n",
      "648/648 [==============================] - 53s 82ms/step - loss: 0.0035\n",
      "Epoch 8/50\n",
      "648/648 [==============================] - 54s 83ms/step - loss: 0.0014\n",
      "Epoch 9/50\n",
      "648/648 [==============================] - 54s 83ms/step - loss: 0.0013\n",
      "Epoch 10/50\n",
      "648/648 [==============================] - 58s 90ms/step - loss: 0.0015\n",
      "Epoch 11/50\n",
      "648/648 [==============================] - 58s 90ms/step - loss: 0.0011\n",
      "Epoch 12/50\n",
      "648/648 [==============================] - 59s 92ms/step - loss: 8.5170e-04\n",
      "Epoch 13/50\n",
      "648/648 [==============================] - 58s 90ms/step - loss: 9.5845e-04\n",
      "Epoch 14/50\n",
      "648/648 [==============================] - 63s 97ms/step - loss: 9.6422e-04\n",
      "Epoch 15/50\n",
      "648/648 [==============================] - 58s 89ms/step - loss: 8.3509e-04\n",
      "Epoch 16/50\n",
      "648/648 [==============================] - 61s 94ms/step - loss: 0.0032\n",
      "Epoch 17/50\n",
      "648/648 [==============================] - 60s 93ms/step - loss: 0.0030 0s - loss: 0\n",
      "Epoch 18/50\n",
      "648/648 [==============================] - 34s 52ms/step - loss: 0.0012\n",
      "Epoch 19/50\n",
      "648/648 [==============================] - 28s 43ms/step - loss: 9.6378e-04\n",
      "Epoch 20/50\n",
      "648/648 [==============================] - 29s 44ms/step - loss: 9.0117e-04\n",
      "Epoch 21/50\n",
      "648/648 [==============================] - 30s 46ms/step - loss: 8.5470e-04\n",
      "Epoch 22/50\n",
      "648/648 [==============================] - 29s 45ms/step - loss: 8.3467e-04\n",
      "Epoch 23/50\n",
      "648/648 [==============================] - 28s 43ms/step - loss: 8.7333e-04\n",
      "Epoch 24/50\n",
      "648/648 [==============================] - 29s 44ms/step - loss: 8.1700e-04\n",
      "Epoch 25/50\n",
      "648/648 [==============================] - 32s 49ms/step - loss: 7.5822e-04 1s - lo\n",
      "Epoch 26/50\n",
      "648/648 [==============================] - 28s 44ms/step - loss: 7.9880e-04\n",
      "Epoch 27/50\n",
      "648/648 [==============================] - 27s 42ms/step - loss: 8.0485e-04\n",
      "Epoch 28/50\n",
      "648/648 [==============================] - 29s 45ms/step - loss: 8.3656e-04\n",
      "Epoch 29/50\n",
      "648/648 [==============================] - 33s 51ms/step - loss: 7.7518e-04\n",
      "Epoch 30/50\n",
      "648/648 [==============================] - 32s 49ms/step - loss: 6.9283e-04\n",
      "Epoch 31/50\n",
      "648/648 [==============================] - 26s 40ms/step - loss: 8.1996e-04\n",
      "Epoch 32/50\n",
      "648/648 [==============================] - 26s 40ms/step - loss: 0.0028\n",
      "Epoch 33/50\n",
      "648/648 [==============================] - 32s 50ms/step - loss: 0.0018\n",
      "Epoch 34/50\n",
      "648/648 [==============================] - 34s 52ms/step - loss: 8.1970e-04\n",
      "Epoch 35/50\n",
      "648/648 [==============================] - 32s 50ms/step - loss: 7.1780e-04\n",
      "Epoch 36/50\n",
      "648/648 [==============================] - 30s 46ms/step - loss: 7.0468e-04\n",
      "Epoch 37/50\n",
      "648/648 [==============================] - 28s 43ms/step - loss: 7.1187e-04\n",
      "Epoch 38/50\n",
      "648/648 [==============================] - 28s 43ms/step - loss: 6.8324e-04\n",
      "Epoch 39/50\n",
      "648/648 [==============================] - 31s 48ms/step - loss: 6.8968e-04\n",
      "Epoch 40/50\n",
      "648/648 [==============================] - 30s 46ms/step - loss: 7.3225e-04\n",
      "Epoch 41/50\n",
      "648/648 [==============================] - 29s 45ms/step - loss: 6.8844e-04\n",
      "Epoch 42/50\n",
      "648/648 [==============================] - 30s 47ms/step - loss: 7.4887e-04\n",
      "Epoch 43/50\n",
      "648/648 [==============================] - 27s 42ms/step - loss: 6.6952e-04\n",
      "Epoch 44/50\n",
      "648/648 [==============================] - 29s 44ms/step - loss: 0.0025\n",
      "Epoch 45/50\n",
      "648/648 [==============================] - 32s 50ms/step - loss: 8.1345e-04\n",
      "Epoch 46/50\n",
      "648/648 [==============================] - 29s 45ms/step - loss: 8.1164e-04\n",
      "Epoch 47/50\n",
      "648/648 [==============================] - 29s 45ms/step - loss: 6.6682e-04\n",
      "Epoch 48/50\n",
      "648/648 [==============================] - 26s 40ms/step - loss: 6.6576e-04\n",
      "Epoch 49/50\n",
      "648/648 [==============================] - 26s 40ms/step - loss: 6.8573e-04\n",
      "Epoch 50/50\n",
      "648/648 [==============================] - 26s 40ms/step - loss: 6.6510e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17b84a46ac8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRU Bidirectional\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_len+1,output_dim=100,input_length=max_len,mask_zero=True))\n",
    "model.add(Bidirectional(GRU(128, activation=\"tanh\")))\n",
    "model.add(Dense(128,activation=\"tanh\"))\n",
    "model.add(Dense(64,activation=\"tanh\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "model.fit(train_sequence_matrix,y_train, batch_size=64,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de2d28ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      8940\n",
      "           1       0.67      0.57      0.62       649\n",
      "\n",
      "    accuracy                           0.95      9589\n",
      "   macro avg       0.82      0.78      0.80      9589\n",
      "weighted avg       0.95      0.95      0.95      9589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_sequence_matrix)\n",
    "y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6564bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
